# Engineering OS — Business & Go-to-Market (Marketplace Draft)

### What this page is

A practical business + go‑to‑market layer that sits next to an engineering checklist.

It helps you turn a build into something people will actually buy, use, and keep—without drifting into “startup theater.” It is written so a beginner can run it.

### What this module produces (deliverables)

- A crisp **Problem/Solution Fit** statement with evidence
- A **JTBD + ICP** profile that guides engineering tradeoffs
- A **competitive map** and positioning
- A **pricing + packaging** hypothesis with validation
- A **demo + story pack** that makes technical value legible
- A lightweight **support/ops plan**
- A **shipping/returns** plan (if physical)

### How to use

Treat each section as supplemental to your build. You do not do everything at once.

You run **small experiments** (“micro‑prototypes”) that reduce the risk of building the wrong thing.

---

## 10‑Minute Quickstart (do this today)

Create a Notion page called **Go‑To‑Market Snapshot** and fill:

1. **One‑sentence value**: “Helps [who] achieve [outcome] by [mechanism] unlike [alternative].”
2. **Target user (ICP v0)**: who, context, budget reality, buying friction
3. **JTBD v0**: “When ***, I want to*** , so I can ___.”
4. **Top three risks**: (a) nobody cares, (b) pricing wrong, (c) adoption friction
5. **Rev A business goal**: “Get X qualified users to do Y and say Z”
6. **Proof asset to build next**: demo video / landing page / pilot install

If you cannot write #1, you are not ready to scale engineering scope.

---

## 1) Problem/Solution Fit

### 1.1 The only thing that matters early

**Problem/Solution Fit** is evidence that a specific group of people repeatedly experiences a specific pain and believes your approach solves it.

Not “cool product.” Not “big market.” Not “good tech.”

---

### 1.2 Define your user and the situation (context beats demographics)

**ICP (Ideal Customer Profile) v0** should include:

- Role/identity (e.g., track-day driver, shop owner, fleet manager)
- Environment constraints (vehicle, noisy shop, outdoors, regulated workplace)
- Skill level (beginner vs power user)
- Budget and purchasing mode (impulse buy vs committee)
- Tooling they already use (ecosystem)
- “Day in the life” triggers (when pain happens)

Rule: if your ICP is “anyone,” your GTM is “no one.”

---

### 1.3 JTBD (Jobs To Be Done) engineers can actually use

Write three versions per job:

- **Functional job**: what they are trying to do
- **Emotional job**: how they want to feel (confidence, control, pride)
- **Social job**: what it signals to others (competence, professionalism)

Format:

- “When I’m ***, I want to*** , so I can ___.”

Then attach:

- Current workaround
- Cost of workaround (time/money/risk)
- What breaks in workaround

This becomes your engineering prioritization anchor.

---

### 1.4 User interviews (the non-cringe method)

The goal is not “do you like it?”

The goal is: how they already solve it and what it costs them.

**Interview script (15–25 min)**

1. Walk me through the last time this happened.
2. What did you do next?
3. What tools did you use?
4. What sucked about that?
5. What would “perfect” look like?
6. What have you tried that didn’t work?
7. If this solved it, what would it be worth? Why? (Ask what they are comparing to.)

**Rules**

- No pitching in the first half.
- Get specifics: timestamps, tools, steps, screenshots if possible.
- Log exact phrases. Those become your copy.

---

### 1.5 Problem proof (how you know it’s real)

You have problem proof when:

- People describe the pain without you leading them
- They have tried multiple workarounds
- They can quantify cost (time wasted, mistakes, money, stress)
- They ask “when can I try it?”

---

### 1.6 Micro‑prototypes (Problem/Solution Fit)

**MP‑H‑PSF‑1: 10 interviews in 10 days**

Success: at least 6/10 report the same pain and can describe workaround cost.

**MP‑H‑PSF‑2: Concierge pilot (manual backend)**

You do it manually behind the scenes to prove value before engineering it.

Success: user repeats usage and says they would be upset if you removed it.

**MP‑H‑PSF‑3: Preorder/deposit test (if appropriate)**

Success: money or commitment appears without heavy persuasion.

**PSF Checklist (minimum viable)**

- [ ]  ICP v0 is narrow and real
- [ ]  10+ interviews logged with quotes
- [ ]  Top three pains ranked by frequency and severity
- [ ]  Workarounds documented with real costs
- [ ]  A must-have outcome is explicit and measurable

---

## 2) Market Sizing & Competitive Map

### 2.1 Market sizing without MBA cosplay

You need sizing for decision-making, not investor decks.

Use three layers:

- TAM: everyone who could ever use it
- SAM: people you can realistically reach
- SOM: the slice you can win in 6–18 months

Builder rule: SOM is what matters for your next release and budget.

---

### 2.2 Competitive map (what “competition” actually is)

Competition includes:

- Direct competitors
- Substitutes (spreadsheets, notebooks, generic tools)
- “Do nothing”
- Internal solutions (DIY rigs, custom scripts)

Create a table of:

- Target user
- Core promise
- Pricing model
- Key differentiators
- Adoption friction
- Where they win/lose

---

### 2.3 Positioning: category + wedge

Pick:

- Category: what box your product sits in (telemetry logger, workflow OS, etc.)
- Wedge: the smallest sharp entry point that gets adoption
- Expansion: what you add once you have users

Rule: most products fail because they try to be “the whole platform” first.

---

### 2.4 Pricing anchors (what users compare to)

People compare your price to:

- Their current workaround cost
- The cheapest acceptable alternative
- The pro tool they have heard of
- The cost of a mistake

Your job is to define the anchor and make it obvious.

---

### Micro‑prototypes (Market/Competition)

**MP‑H‑MKT‑1: Competitive teardown**

Use three competitor products for 30 minutes each.

Success: you can name five “they win here” truths and five “they lose here” truths.

**MP‑H‑MKT‑2: Landing page A/B positioning test**

Two headlines, same product.

Success: higher qualified signups/interview conversions on one positioning.

**Market/Comp Checklist**

- [ ]  SOM defined (not fantasy)
- [ ]  Top five competitors and top five substitutes documented
- [ ]  Your wedge is one clear use case
- [ ]  Pricing anchor identified
- [ ]  Positioning statement written

---

## 3) Brand & Story Pack

### 3.1 The story pack translates engineering value

Most engineers explain features. Buyers buy outcomes.

Story Pack components:

- One-liner value proposition
- Three key outcomes (not features)
- Proof points (evidence, demos, testimonials)
- “How it works” diagram
- Objection handling (price, complexity, trust)
- Demo script (5–7 minutes)

---

### 3.2 Messaging hierarchy (copy you can reuse)

- Headline: outcome + who it is for
- Subhead: how you do it + why it is different
- Three bullets: outcomes
- Proof: evidence, numbers, credibility
- Call-to-action: one step only (try/waitlist/book call)

---

### 3.3 Demo script (engineer-friendly)

A good demo is not a tour. It is a narrative:

1. Problem moment (real scenario)
2. What breaks today (workaround pain)
3. Your product solves it (one workflow)
4. Proof (data/evidence/result)
5. What happens next (CTA)

Rule: demo one killer path, not everything.

---

### Micro‑prototypes (Story)

**MP‑H‑STORY‑1: 5‑minute demo video**

Success: three strangers can explain what it does after watching.

**MP‑H‑STORY‑2: Copy test with real users**

Read headline + bullets. Ask: “What do you think this is? Who is it for?”

Success: they answer correctly without correction.

**Story Pack Checklist**

- [ ]  One-liner is clear
- [ ]  Outcomes > features
- [ ]  Proof exists (screenshots, logs, real numbers)
- [ ]  Demo script is repeatable
- [ ]  Objections answered honestly

---

## 4) Support & Ops Lite

### 4.1 Support is part of the product

Support load is a common killer of small-batch products.

Plan for:

- Onboarding
- Troubleshooting
- Updates
- Returns/replacements (if hardware)
- Feedback loop to backlog

---

### 4.2 Minimum support artifacts

- Quickstart guide (one page)
- FAQ (top 10)
- Troubleshooting decision tree
- Known issues + workarounds
- Contact method + response expectations
- Bug report template (logs + version + repro steps)

---

### 4.3 RMA loop (even at tiny volume)

Define:

- What qualifies for replacement
- Who pays shipping
- How you handle user error
- How you prevent repeats (root cause + fix)

---

### Micro‑prototypes (Support/Ops)

**MP‑H‑OPS‑1: Onboarding dry run**

Give it to a friend. No help allowed.

Success: setup under X minutes and a useful log generated.

**MP‑H‑OPS‑2: Support ticket simulation**

Write five fake tickets and see if your docs solve them.

Success: docs solve at least 3/5 without you.

**Support/Ops Checklist**

- [ ]  Onboarding is self-serve
- [ ]  Bug report template exists
- [ ]  Known issues published
- [ ]  Feedback flows into backlog with tags
- [ ]  RMA policy exists (if physical)

---

## 5) Packaging, Shipping & Returns (physical product path)

### 5.1 Packaging is engineering + cost control

Packaging must handle:

- Drops
- Vibration
- Moisture (sometimes)
- ESD (electronics)
- Labeling requirements
- Unboxing clarity

Rule: the package is part of your reliability story.

---

### 5.2 Shipping design inputs (collect before buying boxes)

- Product dimensions + weight
- Fragile points (connectors, screens)
- ESD sensitivity
- Thermal sensitivity
- Included accessories
- Unboxing steps required for safe setup

---

### 5.3 Returns policy (set expectations early)

Define:

- Return window
- Restocking fees (if any)
- Opened vs unopened policy
- Warranty duration and what it covers
- What counts as misuse

Be clear. Ambiguity becomes support burden.

---

### 5.4 Reverse logistics (tiny scale)

Even if you ship 20 units:

- Track serial numbers
- Track failure mode on returns
- Track whether failure was design vs user setup

This becomes your reliability feedback loop.

---

### Micro‑prototypes (Shipping)

**MP‑H‑SHIP‑1: Drop test packed unit**

Pack and run controlled drop tests.

Success: product survives and remains functional; packaging holds shape.

**MP‑H‑SHIP‑2: Unboxing clarity test**

Hand package to someone and watch.

Success: they do not miss critical steps; setup succeeds.

**Packaging/Shipping Checklist**

- [ ]  Packaging protects fragile areas
- [ ]  ESD precautions included if needed
- [ ]  Labeling is correct (input ratings, warnings)
- [ ]  Return/warranty policy written
- [ ]  Serial tracking exists (even a spreadsheet)

# Engineering OS — Business & Governance (Marketplace Draft)

### What this page is

A compact, practical set of operating systems for **business-side execution** in an Engineering OS.

It defines three databases (21–23), each with:

- **Purpose** and **Definition of Done**
- Suggested **Fields** (schema)
- A **CSV Starter** for quick import
- A lightweight **SOP** and **Gate criteria**
- Clear **Relations** for traceability

### How to use

1. Treat each section (21–23) as its own database.
2. Create the database using the **Fields** list.
3. Optional: copy the **CSV Starter** into a `.csv` and import it.
4. Wire the **Relations** so cost, changes, releases, and comms stay connected.

---

## Engineering OS — Business & Governance (21–23)

## 21) Change Control (ECR/ECO/Deviation/Waiver)

**Purpose**: Control changes after baselines to prevent scope and quality drift, while preserving traceability.

**Definition of Done**: Each change includes impact analysis, approvals, updated artifacts and baselines, a verification plan, and a communication record.

**Fields**

- Change ID (title)
- Type (select: ECR – request, ECO – order, Deviation, Waiver)
- Affected Items (relations: Requirements, ICDs, ADRs, BOM items, Tests, Releases, Baselines)
- Reason/Problem Statement (text)
- Impact Analysis
    - Multi-select: Safety, Quality, Cost, Schedule, Compliance
    - Notes (text)
- Risk Level (select: Low, Medium, High)
- Proposed Change (text)
- Verification Needed (relation or links to Tests/Procedures)
- Approvals (people)
- Approval Date(s) (date)
- Status (select: Proposed, Under Review, Approved, Implemented, Verified, Closed)
- Effective Version/Date (text/date)
- Backout/Rollback Plan (text)
- Communications Done (checkbox)
- Comms Link (url or relation to announcement/runbook)

**CSV Starter**

```
Change ID,Type,Affected Items,Reason/Problem Statement,Impact Analysis,Risk Level,Proposed Change,Verification Needed,Approvals,Status,Effective Version/Date,Backout/Rollback Plan,Communications Done
ECR-0001,ECR,"R-F-002; ICD:Primary Bus","Display glitches in sun","Quality; Schedule","Medium","Switch to high-nit panel; update ICD","T-012 daylight readability; T-013 thermal soak","PM; Tech Lead","Under Review",,"Revert to prior panel; ICD v1.2","FALSE"

```

**SOP**

1. Submit an **ECR** with a clear reason and affected items. Attach evidence (defects, test results).
2. Perform **impact analysis** (cost, schedule, quality, safety, compliance). Assign a risk level and list impacted stakeholders.
3. Run a review (can be a solo + 1 reviewer board). Record the decision: Approve, Reject, or Revise.
4. If approved, convert to **ECO** and update artifacts (ICD, drawings, code, BOM). Re-baseline if needed.
5. Link required tests and execute verification. Attach evidence.
6. Communicate to downstream consumers. Update runbooks. Mark **Communications Done**.
7. Close only when implemented and verified. Capture lessons in a knowledge base (optional).

**Gates**

- **Entrance**: Problem statement + evidence + affected artifacts listed.
- **Exit**: Baseline updated. Verification passed. Rollback plan documented. Stakeholders notified.

**Relations**: Requirements, ICDs, ADRs, BOM, Tests, Releases, Baselines, Runbooks.

---

## 22) Budget & Cost Model (NRE, Unit Economics, Forecast)

**Purpose**: Keep feasibility grounded. Track NRE, CapEx/OpEx, unit cost, margins, and variance vs plan.

**Definition of Done**: A baseline budget exists, a live forecast is maintained, and unit economics are validated at CDR and PRR.

**Fields**

- Line Item (title)
- Category (select: NRE, CapEx, OpEx, Unit BOM, Tooling, Test, Travel, Contingency)
- Project/Phase (select: Idea, Planning, Build, V&V, Release, Ops)
- Amount – Planned (number, currency)
- Amount – Actual (number, currency)
- Variance (formula: Actual − Planned)
- Assumptions (text)
- Vendor/PO (text or relation to Procurement)
- Funding Source (select or text: self, angel, grant, internal)
- Unit Cost Model (relation to “Unit Economics” DB, or inline fields below)
- Last Updated (date)

**Optional Unit Cost Fields** (same DB or a related “Unit Economics” DB)

- Target MSRP (number)
- Target Margin % (number)
- Target Unit Cost (number)
- BOM Cost (rollup from BOM)
- Assembly Labor (number)
- Overhead/Allocated (number)
- Current Estimated Unit Cost (formula)
- Delta vs Target (formula)

**CSV Starter**

```
Line Item,Category,Project/Phase,Amount – Planned,Amount – Actual,Assumptions,Vendor/PO,Funding Source,Last Updated
Prototype PCBs,NRE,Build,600,0,"Qty 10, 4-layer, std lead","JLC-PO-1023",Self,2025-01-10
Displays (10x),Unit BOM,Build,350,0,"$35 each","PO-1004",Self,2025-01-10
Enclosure Tooling,Tooling,Build,1200,0,"Single-cavity, 3D print pilot","Vendor TBD",Self,2025-01-10

```

**SOP**

1. Baseline the budget at PDR. Include NRE, materials, and a contingency (10–20% is typical).
2. Forecast monthly. Link Procurement POs. Update actuals and variance.
3. At CDR, validate unit economics (target unit cost vs BOM + labor + overhead).
4. At PRR, confirm burn vs plan and document funding coverage for the next phase.
5. When scope changes via ECO, update the budget and unit economics. Record the delta and rationale.

**Gates**

- **CDR Exit**: Unit cost within target ±X%. Contingency ≥ threshold. Funding secured.
- **PRR Exit**: Variance within tolerance. Re-forecast approved. Release fully costed.

**Relations**: Procurement, BOM, Releases, Projects, Change Control.

---

## 23) Go-To-Market (GTM) & Stakeholder Communications

**Purpose**: Ensure adoption with clear value propositions, launch assets, and feedback loops for users, stakeholders, support, and ops.

**Definition of Done**: Persona map, messaging, assets, launch checklist, success metrics, and a post-launch review are in place.

**Fields**

- Campaign/Launch Name (title)
- Audience Persona (multi-select: End User, Exec/VC, Ops/Support, Integrator, QA/Compliance)
- Core Message / Value Prop (text)
- Proof/Evidence (links: test data, demos, case studies)
- Channels (multi-select: Email, Deck, README/Docs, Video, Blog, In-App, Social)
- Assets (relations/links: Owner’s Guide, Deck, One-Pager, Demo Script, FAQ)
- Launch Date / Window (date)
- Success Metrics
    - Multi-select: Adoption, Activation, Retention, NPS, MTTR
    - Targets (text)
- Risks/Dependencies (text)
- Owner (person)
- Status (select: Planned, Ready, Launched, Closed)
- Post-Launch Review (text; link defects/feedback)

**CSV Starter**

```
Campaign/Launch Name,Audience Persona,Core Message / Value Prop,Proof/Evidence,Channels,Assets,Launch Date / Window,Success Metrics,Risks/Dependencies,Owner,Status
Rev A Launch,"End User; Exec/VC","Trustworthy telemetry with simple install","/verif-report.pdf; /[demo-video.mp](http://demo-video.mp)4","Deck; Docs; Video; Blog","/[owners-guide.md](http://owners-guide.md); /one-pager.pdf; /pitch-deck.pptx",2025-02-20,"Adoption; Activation","Supply for displays; firmware freeze","Kian","Planned"

```

**SOP**

1. Build a persona map. For each audience, define jobs-to-be-done, objections, and win conditions.
2. Write the message and attach proof: verification plots, demos, and case studies.
3. Produce the core assets: Owner’s Guide, one-pager, deck, demo script, FAQ. Assign owners.
4. Plan channels and calendar. Align with the release version and runbooks. Dry-run the demo.
5. Launch and monitor success metrics. Route issues into Defects. Collect feedback.
6. Run a post-launch review. Update assets. Feed learning into Requirements and ADRs for Rev B.

**Gates**

- **PRR Entrance**: Assets ready. Demo rehearsed. Success metrics and targets set.
- **Launch Exit**: Metrics captured. Issues logged. Stakeholder comms complete. Post-launch review done.

**Relations**: Releases, Owner’s Guide/Docs, Tests (evidence), Defects, Projects.

---

## Cross-links to add (wire once)

- **Change Control** ↔ Requirements, ICDs, ADRs, BOM, Tests, Releases, Baselines.
- **Budget & Cost Model** ↔ Procurement, BOM, Releases, Projects, Change Control.
- **GTM & Stakeholder Comms** ↔ Releases, Docs (Owner’s Guide/FAQ), Tests (evidence), Defects.

# Engineering OS — Operations & Governance (24–30) (Marketplace Draft)

### What this page is

A marketplace-ready, Notion-native spec for **Operations & Governance (24–30)** inside an Engineering OS.

Each section is intended to become a dedicated database (or page + linked database), and includes:

- **Purpose** and **Definition of Done**
- Suggested **Fields** (schema)
- A **CSV Starter** for quick import
- A pragmatic **SOP** and **Gates**
- **Relations** to keep execution traceable end-to-end

### How to use

1. Treat each section (24–30) as its own database.
2. Create the database using the **Fields** list.
3. Optional: copy the **CSV Starter** into a `.csv` and import it.
4. Wire the **Relations** at the end once, then reuse across projects.

---

## Engineering OS — Operations & Governance (24–30)

## 24) Procurement & Supply Chain

**Purpose**: Source the right parts and services at the right cost and lead time, with alternates and traceability.

**Definition of Done**: RFQs compared, supplier selected, PO issued, tracking active, receipts logged, and links to BOM and budget updated.

**Fields**

- PO ID (title)
- Item / Spec / PN (text; link to BOM item)
- Supplier (relation to “Suppliers” DB)
- Status (select: Draft, RFQ, Ordered, Shipped, Received, Closed)
- Qty (number)
- Unit Cost (currency)
- Extended Cost (formula)
- Lead Time (text)
- MOQ / NCNR (text)
- Alt Sources (text or relations)
- Incoterms/Ship Method (text)
- Expected Receipt (date)
- Tracking / Link (url)
- Received Date (date)
- Receiver / Inspection Lot (relation)
- Notes / Risks (text)

**CSV Starter**

```
PO ID,Item / Spec / PN,Supplier,Status,Qty,Unit Cost,Lead Time,MOQ / NCNR,Alt Sources,Incoterms/Ship Method,Expected Receipt,Tracking / Link,Received Date,Receiver / Inspection Lot,Notes / Risks
PO-0001,"Display, 2.4in, SPI, 400nit",Vendor A,Ordered,10,34.50,"2 wks","MOQ 5; NCNR","Vendor B; Vendor C","DAP; DHL",2025-02-05,[https://tracking.example](https://tracking.example),,,

```

**SOP**

1. RFQ to ≥2 suppliers for critical items. Capture lead time and terms.
2. Compare total landed cost and risk. Choose a primary and an alternate.
3. Issue PO with spec and revision. Link to BOM and budget.
4. Track shipment and keep ETA current.
5. Receive and create an incoming inspection lot. Resolve NCRs.
6. Close PO and update actuals. Record supplier performance notes.

**Gates**

- **Build start**: All critical-path POs placed. Alternates identified.
- **Assembly start**: Critical lots received and dispositioned.

**Relations**: BOM, Budget, Suppliers, Incoming Inspection, Change Control, Schedule.

---

## 25) Supplier Quality & Incoming Inspection

**Purpose**: Ensure received parts meet spec. Track supplier performance and corrective actions.

**Definition of Done**: Every receipt has an inspection record (plan, sample, results) and disposition. Supplier scorecards are maintained.

**Fields (Suppliers)**

- Supplier (title)
- Type (select: Distributor, CM, ODM, Service)
- Approved (checkbox)
- Score – OTD (number)
- Score – Quality (number)
- Audits / Notes (text)
- Contacts / Agreements (files/links)

**Fields (Incoming Inspection Lots)**

- Lot ID (title)
- PO ID (relation)
- Part Number (relation to BOM)
- Supplier (relation)
- Qty Received (number)
- Sample Size / Plan (text)
- Checks / Measurements (text/files)
- Result (select: Pass, Fail)
- NCR # (text)
- Disposition (select: Use as-is, Rework, Return, Scrap)
- CAPA Actions (text; relation to Defects/CAPA)
- Inspector (person)
- Date (date)

**CSV Starter (Lots)**

```
Lot ID,PO ID,Part Number,Supplier,Qty Received,Sample Size / Plan,Checks / Measurements,Result,NCR #,Disposition,CAPA Actions,Inspector / Date
LOT-102,PO-0001,DISP-2.4-SPI,Vendor A,10,"AQL 1.0; n=3","Luminance; SPI ID; Pixel test",Pass,,Use as-is,,2025-02-05

```

**SOP**

1. Define inspection plans by part criticality (visual, dimensional, functional).
2. Receive and quarantine. Inspect per plan. Attach evidence.
3. If failed, issue an NCR. Set disposition. Raise CAPA.
4. Score suppliers monthly (OTD and quality). Update approval status.

**Gates**

- **Approved supplier** required before PO for critical parts.
- **Lot release** only after pass or explicit disposition is recorded.

**Relations**: Procurement/POs, BOM, Defects/CAPA, Budget, Risk Register.

---

## 26) Security, Privacy & Safety (Threats, SBOM, Hazards)

**Purpose**: Identify and mitigate product risks across security, privacy/data handling, and physical safety.

**Definition of Done**: Asset and data map, threat model, SBOM, hazard analysis, mitigations, and verification evidence are baselined at release.

**Fields**

- Risk Item (title)
- Category (select: Security, Privacy, Safety)
- Asset/Data/Hazard (text)
- Context/Assumptions (text)
- Threat/Hazard Method (select/text: STRIDE, FMEA, STPA, etc.)
- Severity (select: Low, Medium, High, Catastrophic)
- Likelihood (select: Low, Medium, High)
- Risk Rating (formula)
- Controls/Mitigations (text)
- Verification (links to tests/reports)
- Owner (person)
- Due (date)
- Residual Risk (text; accept/transfer/mitigate)
- SBOM Link (url/file)
- Status (select: Open, Mitigated, Accepted, Closed)

**CSV Starter**

```
Risk Item,Category,Asset/Data/Hazard,Context/Assumptions,Threat/Hazard Method,Severity,Likelihood,Controls/Mitigations,Verification,Owner / Due,Residual Risk,SBOM Link,Status
Unauthorized data access,Security,"Logs; user IDs","Local storage","STRIDE",High,Med,"Encrypt at rest; access control","Pen test report; unit test",Owner/2025-02-15,"Low (accepted)",[https://repo/sbom.json,Mitigated,Open,Open](https://repo/sbom.json,Mitigated,Open,Open)
Thermal burn,Safety,"Enclosure hot surface","Max 60C cabin","FMEA",Med,Low,"Derate load; vents; warning in guide","Thermal soak test",Owner/2025-02-20,"Very Low",,Open

```

**SOP**

1. Build an asset and data map. Classify sensitivity.
2. Run a threat model (STRIDE) and produce an SBOM. Scan dependencies.
3. Run safety analysis (FMEA/DFMEA/HAZOP). Define controls and labels.
4. Verify mitigations (tests and scans). Record residual risk and sign-off.
5. Track vulns and hazards. Feed into Change Control and Releases.

**Gates**

- **Release**: SBOM published. Critical vulns triaged. Safety controls verified. Privacy notice documented.

**Relations**: Requirements (security/safety), Tests, Releases, Docs/Owner’s Guide, Change Control, Risk Register.

---

## 27) Release & Configuration Management

**Purpose**: Ship reproducible versions with frozen baselines, clear notes, and rollback.

**Definition of Done**: Versioned release includes artifacts, change list (ECOs), verification evidence, instructions, and a rollback plan.

**Fields**

- Release ID / Version (title)
- Date (date)
- Scope / Highlights (text)
- Included Changes (relations to ECR/ECO/Defects)
- Baselines (ICD vX, Drawings vY, Firmware tag, BOM rev)
- Artifacts (files/urls: binaries, Gerbers, STLs, docs)
- Checklists (Pre-Release, Release, Post-Release)
- Verification Evidence (links to tests/reports)
- Rollback Plan (text)
- Approvals (people)
- Status (select: Planned, Released, Deprecated)

**CSV Starter**

```
Release ID / Version,Date,Scope / Highlights,Included Changes,Baselines,Artifacts,Checklists,Verification Evidence,Rollback Plan,Approvals,Status
RevA-1.0,2025-03-01,"Initial GA","ECO-12; DEF-44 closed","ICD v1.2; BOM r3; FW v1.0","/artifacts/[gerbers.zip](http://gerbers.zip); /fw/fw_1.0.bin; /owners_guide.pdf","/checklists/[release.md](http://release.md)","/reports/verif_revA.pdf","Revert to RevA-rc4; notify users","PM; Tech Lead",Released

```

**SOP**

1. Document branch/tag strategy. Freeze scope. Bump version.
2. Run the release checklist. Collect artifacts and evidence. Update baselines.
3. Publish artifacts and notes. Execute comms. Update ops runbooks.
4. Monitor metrics and defects. Decide hotfix vs next minor.

**Gates**

- **PRR**: Checklists complete. Artifacts reproducible. Rollback defined.
- **Post-release**: Issues triaged. Metrics reviewed. Deprecations planned.

**Relations**: Change Control, Tests, Docs, GTM, Ops/Runbooks, Budget, Security/SBOM.

---

## 28) Operations, Support & Incident Response

**Purpose**: Keep the product reliable post-release and make response predictable.

**Definition of Done**: Runbooks, SLOs and alerts, escalation path, incident process, and postmortems are defined and used.

**Fields (Runbooks)**

- Runbook ID (title)
- Service/Subsystem (text)
- Start/Stop/Deploy Procedures (text)
- Dependencies (text/relations)
- SLOs/SLAs (text)
- Health Checks/Monitors (text)
- Escalation (contacts/hours)
- Known Issues / Workarounds (text)
- Last Test (date)

**Fields (Incidents)**

- Incident ID (title)
- Severity (select: SEV1, SEV2, SEV3, SEV4)
- Start (date/time)
- End (date/time)
- Impact / Symptom (text)
- Root Cause (text)
- Fix Applied (text)
- Follow-ups (tasks/owners/dates)
- Status (select: Open, Monitoring, Closed)
- Links (release, defects, metrics)

**CSV Starter (Incidents)**

```
Incident ID,Severity,Start,End,Impact / Symptom,Root Cause,Fix Applied,Follow-ups,Status,Links
INC-0007,SEV2,"2025-03-10 10:22","2025-03-10 11:05","Logging stops intermittently","SD write contention","Buffered writes + retry","Add watchdog test; doc known issue",Closed,"Rel RevA-1.0; DEF-77"

```

**SOP**

1. Write runbooks for normal operations and emergency procedures.
2. Define SLOs and map monitors directly to SLOs.
3. Define on-call coverage (solo still means defined hours and escalation).
4. Handle incidents: declare, mitigate, record, and write a postmortem within 72 hours.
5. Feed learnings into Requirements, ECOs, and test coverage.

**Gates**

- **Before GA**: Runbooks, SLOs, and monitors exist. Dry-run an incident.
- **Close incident**: Root cause found. Follow-ups assigned and tracked.

**Relations**: Releases, Defects, Tests, Docs/Runbooks, Metrics, Change Control.

---

## 29) Metrics, Analytics & Experimentation

**Purpose**: Make decisions with data (adoption, reliability, efficiency) and test changes safely.

**Definition of Done**: North-star plus supporting metrics defined, pipeline documented, dashboards live, experiments logged with decisions.

**Fields (Metrics)**

- Metric (title)
- Type (select: Leading, Lagging)
- Definition (exact formula)
- Source (select/text: log, DB, survey)
- Owner (person)
- Target (number/threshold)
- Cadence (select: Daily, Weekly, Release)
- Dashboard Link (url)

**Fields (Experiments)**

- Experiment ID (title)
- Hypothesis (text)
- Variant(s) (text)
- Primary Metric (relation)
- Guardrails (relations)
- Exposure / Duration (text)
- Result (select: Win, Loss, Inconclusive)
- Decision (select: Ship, Hold, Revisit)
- Notes (text)

**CSV Starter (Metrics)**

```
Metric,Type,Definition,Source,Owner,Target,Cadence,Dashboard Link
Activation Rate,Leading,"Activated/Signups","App telemetry",Owner,0.40,Weekly,[https://dash/activation](https://dash/activation)
MTTR,Lagging,"Mean time to restore (mins)","Incident DB",Owner,30,Monthly,[https://dash/ops](https://dash/ops)

```

**SOP**

1. Define a north-star metric and 3–5 supporting metrics. Write exact formulas.
2. Instrument collection and validate accuracy. Respect privacy.
3. Maintain dashboards on a cadence. Assign owners.
4. Pre-register experiments with hypothesis and metrics. Run, decide, and document.
5. Close the loop: changes → metrics → roadmap updates.

**Gates**

- **Pre-launch**: Minimum metric set is live and validated.
- **Post-launch**: Weekly metric review. Experiments gated on guardrails.

**Relations**: Requirements, Releases, Ops/Incidents, GTM, Security/Privacy.

---

## 30) IP, Legal & Compliance

**Purpose**: Protect IP, honor licenses, and meet regulatory obligations (labels, testing, export, privacy).

**Definition of Done**: IP log, OSS compliance (SBOM + notices), regulatory plan/tests, privacy posture, and export classification are maintained and auditable.

**Fields**

- Item (title)
- Category (select: Patent, Trademark, Copyright, OSS License, Regulatory, Export, Data Privacy)
- Description (text)
- Jurisdiction / Standard (text: USPTO, CE/FCC/UL, ISO, GDPR, CCPA, EAR/ITAR)
- Status (select: Draft, Filed, Approved, Compliant, Not Required)
- Evidence/Links (files/urls)
- Counsel/Contact (text)
- Deadlines/Expiry (date)
- Impacted Artifacts (relations: Releases, Docs, BOM)
- Notes / Obligations (text)

**CSV Starter**

```
Item,Category,Description,Jurisdiction / Standard,Status,Evidence/Links,Counsel/Contact,Deadlines/Expiry,Impacted Artifacts,Notes / Obligations
Product Name,Trademark,"Word mark class 009",USPTO,Draft,,Law Firm TBD,2025-06-01,"RevA-1.0","Clearance search before launch"
Open Source Notices,OSS License,"Notices for bundled libs",SBOM/Notices,Compliant,[https://repo/sbom.json,,,"RevA-1.0](https://repo/sbom.json,,,"RevA-1.0); Docs","Include NOTICE file in distro"
EMC/Radio,Regulatory,"Unintentional radiator test",FCC Part 15,Planned,,Test Lab TBD,2025-05-15,"Hardware RevA","Labeling and user manual statements"

```

**SOP**

1. Run an IP sweep at PDR and CDR. Record patentable items. Run trademark clearance.
2. Maintain OSS compliance: SBOM, license checks, notices, and attributions.
3. Build the regulatory plan (EMC, safety, radio, medical, etc.). Schedule lab testing and archive reports.
4. Privacy: data map, notices, request handling process, retention policy.
5. Export: classify product, restrict where needed, record ECCN.

**Gates**

- **PRR**: OSS and labeling obligations met. Regulatory plan scheduled.
- **Release**: Required labels and manual text included. Compliance reports archived.

**Relations**: Security/SBOM, Releases, Docs, Procurement (licenses), GTM, Risk Register.

---

## Cross-links to add (wire once)

- **Procurement** ↔ BOM, Budget, Suppliers, Inspection, Schedule.
- **Supplier Quality** ↔ Procurement, BOM, Defects/CAPA, Risk.
- **Security/Privacy/Safety** ↔ Requirements, Tests, Releases, Docs, IP/Compliance.
- **Release & Config** ↔ Change Control, Tests, Docs, GTM, Ops.
- **Ops & Incidents** ↔ Releases, Metrics, Runbooks, Defects.
- **Metrics & Experiments** ↔ Requirements, Releases, Ops, GTM.
- **IP/Legal/Compliance** ↔ Security/SBOM, Docs, Releases, Procurement.

# Engineering OS — Scale, Quality & Lifecycle (31–35) (Marketplace Draft)

### What this page is

A marketplace-ready, Notion-native spec for **Scale, Quality & Lifecycle (31–35)** inside an Engineering OS.

Each section is intended to become a dedicated database (or page + linked database), and includes:

- **Purpose** and **Definition of Done**
- Suggested **Fields** (schema)
- A **CSV Starter** for quick import
- A pragmatic **SOP** and **Gates**
- **Relations** to preserve traceability across manufacturing, quality, releases, and support

### How to use

1. Treat each section (31–35) as its own database.
2. Create the database using the **Fields** list.
3. Optional: copy the **CSV Starter** into a `.csv` and import it.
4. Wire the relations once so changes, tests, pilot builds, and releases stay connected.

---

## Engineering OS — Scale, Quality & Lifecycle (31–35)

## 31) DFM/DFA/DFS — Design for Manufacture, Assembly & Service

**Purpose**: Make the product easy to build, assemble, test, repair, and upgrade, with minimal cost and variability.

**Definition of Done**: Build, assembly, and service plans exist. Cycle time, required tools/fixtures, CTQs, and service access are defined. 100% of parts have assembly instructions or drawings. First-article build issues are closed.

**Fields**

- Item (title)
- DFM notes (text)
- DFA notes (text)
- DFS notes (text)
- Assembly Steps (relation to “Work Instructions”)
- Required Tools/Fixtures (text)
- Takt/Cycle Time (mins)
- CTQs & Checks (text)
- Service Interval/MTTR target (mins)
- Spares/Repair Parts (relation to BOM)
- Status (select: Draft, Ready, Verified)
- Evidence (photos/files/links)

**CSV Starter**

```
Item,DFM notes,DFA notes,DFS notes,Required Tools/Fixtures,Takt/Cycle Time,CTQs & Checks,Service Interval/MTTR target,Status,Evidence
Main Assembly,"Consolidate fasteners to 2 sizes","Keyed connectors; poka-yoke bracket","Front access to fuses; no potting","#2 driver; 3D-printed nest",6,"Torque 0.5–0.6 Nm; label scan","Fuse swap MTTR < 5 min",Draft,/links/dfa_[pack.md](http://pack.md)

```

**SOP**

1. **DFM pass**: reduce unique parts and fasteners. Specify tolerances and finishes your supplier can reliably hold.
2. **DFA pass**: define assembly order, ergonomics, and mistake-proofing (poka-yoke). Confirm single-operator feasibility.
3. **DFS pass**: define wear parts, access clearances, MTTR targets, and repair level (board vs module vs unit).
4. Author Work Instructions with photos and checks. Record takt time.
5. Run a dry build, log issues, close them via ECOs, and update instructions.

**Gates**

- **Before pilot**: DFM/DFA/DFS review signed. First-article build plan approved.
- **Pre-release**: MTTR and assembly CTQs met on sample builds.

**Relations**: BOM, Work Instructions, Quality Plan, Pilot Build, Change Control, Releases.

---

## 32) Pilot Build & Beta Program Management

**Purpose**: Validate buildability and real-world use with limited units and users before full release.

**Definition of Done**: Pilot lot built using final instructions. Beta testers recruited and qualified. Feedback and defects tracked. Go/no-go criteria satisfied.

**Fields (Pilot Lots)**

- Pilot Lot ID (title)
- Qty (number)
- Build Site (text)
- WI/Rev used (relation)
- Known Deviations (text)
- Issues Found (relation to Defects)
- Yield (percentage)
- Lessons Learned (text)
- Status (select: Planned, In Build, Complete)

**Fields (Beta Cohort)**

- Tester ID (title)
- Segment/Use Case (text)
- Agreement/Safety Ack (file/link)
- Device/Build Assigned (relation)
- Feedback Cadence (select: Weekly, Biweekly, End-of-run)
- Issues Logged (relation to Defects)
- NPS/CSAT (number)
- Status (select: Active, Paused, Complete)

**CSV Starter (Pilot)**

```
Pilot Lot ID,Qty,Build Site,WI/Rev used,Known Deviations,Issues Found,Yield,Lessons Learned,Status
PILOT-01,20,"In-house","WI-Assy r2.1","Alt fastener used","DEF-110; DEF-117",90,"Add torque stickers; simplify step 4",Complete

```

**SOP**

1. Define objectives (yield, CTQ hit rate, field robustness hypotheses).
2. Build the pilot with production-like process. Record yield and defects.
3. Recruit beta testers. Capture consent and safety acknowledgement. Set cadence and exit criteria.
4. Instrument feedback (surveys, logs, interviews). Triage defects and route ECOs.
5. Review go/no-go criteria and push learnings into DFM/DFA, work instructions, and the roadmap.

**Gates**

- **Pilot exit**: Yield ≥ target, top CTQs passed, critical defects closed or mitigated.
- **Beta exit**: Go/no-go criteria met (reliability, UX, safety). Support playbooks proven.

**Relations**: DFM/DFA/DFS, Work Instructions, Defects/CAPA, Releases, Ops/Runbooks, Metrics.

---

## 33) Reliability Engineering (HALT/HASS, Durability & Environmental)

**Purpose**: Prove the product survives intended environments and reasonable abuse. Find margin early.

**Definition of Done**: Reliability plan approved. Stress profiles defined. Test results include failure analysis and corrective actions. Reliability metrics (e.g., MTTF/AFR) estimated.

**Fields**

- Test ID (title)
- Profile/Standard (IEC, MIL-STD, JEDEC, custom)
- Stress Type (select: Thermal, Vibration, Shock, Humidity, Power Cycling, Soak)
- Levels & Duration (text)
- Sample Size (number)
- Acceptance Criteria (text)
- Results (select: Pass, Fail, Mixed)
- Failure Mode/FA Report (files/links)
- Corrective Action (text/relation)
- Status (select: Planned, Running, Complete)

**CSV Starter**

```
Test ID,Profile/Standard,Stress Type,Levels & Duration,Sample Size,Acceptance Criteria,Results,Failure Mode/FA Report,Corrective Action,Status
REL-TH-01,JEDEC JESD22-A104,Thermal Cycling,"-20↔70°C, 30 min dwells, 200 cycles",6,"No functional loss; enclosure intact",Mixed,/fa/rel_th_01.pdf,"Change gasket material; conformal coat",Complete

```

**SOP**

1. Define mission profile (temperature, duty cycle, vibration, power conditions).
2. Select tests (HALT/HASS, thermal/vibe/shock/humidity, ESD/EMI pre-scan).
3. Run tests and capture failures. Perform failure analysis (microscopy, logs, X-ray, etc.).
4. Implement fixes and re-test to verify margin. Update requirements if needed.
5. Model reliability (Arrhenius/Coffin-Manson where applicable) and publish claims conservatively.

**Gates**

- **CDR**: Reliability plan baselined. Fixtures and providers ready.
- **PRR/Release**: Critical reliability tests pass or mitigations accepted with rationale.

**Relations**: Requirements (environmental), Risk Register, Quality Plan, Change Control, Releases.

---

## 34) Value Engineering & Cost Reduction

**Purpose**: Reduce unit cost and complexity without hurting CTQs, safety, or reliability.

**Definition of Done**: Cost tree built. Top cost drivers identified. Candidates evaluated (DFX, alternate sourcing, integration). Savings realized and validated by test.

**Fields**

- Idea ID (title)
- Cost Driver (select: Part, Process, Logistics, Overhead)
- Current Cost (currency)
- Proposed Change (text)
- Est. Savings (currency/% )
- Impact on CTQs (text)
- Validation Required (tests/analysis)
- Decision (select: Proceed, Hold, Reject)
- Owner (person)
- Due (date)
- Status (select: Open, Executing, Realized, Closed)
- Proof of Savings (links/files)

**CSV Starter**

```
Idea ID,Cost Driver,Current Cost,Proposed Change,Est. Savings,Impact on CTQs,Validation Required,Decision,Owner / Due,Status,Proof of Savings
VE-07,Part,"$12.40 (display)","Switch to 320-nit with AR film","$3.10 (-25%)","Readability may drop","Sunlight readability test; user trial",Proceed,Owner/2025-03-15,Executing,/reports/ve_07.pdf

```

**SOP**

1. Build a cost tree (BOM, process, logistics). Rank the top 10 drivers.
2. Generate options: consolidate parts, reduce operations, alternate materials/vendors, integrate modules.
3. Score savings vs risk vs CTQ impact. Pick pilots.
4. Validate against CTQs, reliability, and compliance.
5. Implement through ECOs. Verify savings on real POs. Update pricing and margin model.

**Gates**

- **Pre-ECO**: Validation plan approved. CTQs explicitly listed.
- **Close**: Savings realized in purchasing and no CTQ regression.

**Relations**: BOM, Procurement, Quality/CTQs, Tests, Change Control, Finance/Margins, Releases.

---

## 35) End-of-Life (EOL), Sunsetting & Migration

**Purpose**: Retire products responsibly while protecting users, data, brand, and compliance.

**Definition of Done**: EOL policy and plan published. Last-time-buy managed. Support and migration path documented. Archival and legal holds satisfied.

**Fields**

- EOL ID (title)
- Scope/Versions (text)
- Rationale (select/text: Obsolete, Low demand, Parts EOL, Strategic)
- Key Dates (Announce / LTB / Support End)
- Customer Impact & Comms (text/files)
- Migration Path (successor/upgrade/adapter)
- Data Export/Retention Plan (text)
- Environmental/Disposal Guidance (text)
- Spare Parts Strategy (qty/years)
- Legal/Contractual Obligations (text)
- Status (select: Planned, Announced, Executing, Complete)
- Links (Releases, Docs, Legal, Suppliers)

**CSV Starter**

```
EOL ID,Scope/Versions,Rationale,Key Dates,Customer Impact & Comms,Migration Path,Data Export/Retention Plan,Environmental/Disposal Guidance,Spare Parts Strategy,Legal/Contractual Obligations,Status,Links
EOL-2026-01,"RevA ≤1.2","Parts EOL","Announce 2026-04-01; LTB 2026-06-01; Support End 2027-06-01","Email + site notice + in-product banner","Successor RevB + adapter","Self-serve export; 12-mo retention after end","WEEE guidance; recycling partners","Critical spares for 24 months","Per enterprise contracts: 12-mo notice",Planned,"/docs/eol_[plan.md](http://plan.md)"

```

**SOP**

1. Decide EOL with rationale. Align with legal and major customers.
2. Publish timeline (announce → LTB → end of support) and provide FAQs.
3. Define migration (successor, adapters, credits) and provide data export tools.
4. Plan spares and repair commitments. Document safe disposal.
5. Archive artifacts. Close security updates per policy. Run a final postmortem.

**Gates**

- **Announce**: Comms and migration guide ready. Customer success briefed.
- **Complete**: Support obligations met. Archives and exports closed. Legal satisfied.

**Relations**: Releases, Docs/Owner’s Guide, Security/Privacy, Procurement (LTB), Finance, Legal/Compliance, Ops/Support.

# Engineering OS — Human Systems & Personal Ops (Marketplace Draft)

### What this page is

A practical **human operating system** for building—solo or with a team—without burning out, thrashing, or becoming the bottleneck.

This is not motivation. It is execution support:

- Time structure that protects deep work
- Collaboration patterns that reduce context hoarding
- Postmortems that prevent repeat failures
- Communication templates that make progress legible

### What this module produces (deliverables)

- A repeatable weekly operating cadence
- A solo-to-team transition kit (handoffs + onboarding)
- A postmortem system that improves reliability over time
- A communication pack (status updates, demos, evidence visuals)
- Personal anti-chaos rules that protect decision quality

---

## 10‑Minute Quickstart (do this today)

Create a Notion page called **Personal Ops Control Panel** with:

1. Weekly Review template (15–30 minutes, recurring)
2. Focus block rules (what counts as deep work, what does not)
3. Current bottleneck (one sentence)
4. Top three risks (from Module A) + what you will do this week to burn them down
5. Status update template (copy/paste ready)

Then adopt one constraint immediately:

- No more than three active tasks at once (WIP limit). If you violate this, your schedule becomes fiction.

---

## 1) Solo‑to‑Team Transition

**Scope**: handoffs, onboarding kits, role slicing, collaboration.

### 1.1 The problem this solves

Solo builders naturally hoard context. When someone joins, everything slows down because:

- They do not know where anything is
- They do not know what “done” means
- They cannot reproduce your setup
- You become the reviewer, tester, and decision-maker for everything

Goal: make yourself replaceable without losing quality.

---

### 1.2 Minimum Onboarding Kit (for any contributor)

This kit lives in Notion and links to repo folders.

Onboarding kit contents:

- Project one-liner + purpose
- Current phase (Rev A/Rev B) + goals
- System overview diagram (block diagram)
- Repo structure + how to build/run/flash
- Toolchain “Known Good” (Module I)
- Top risks + mitigation plan (Module A)
- Current milestone + pass criteria (Module F)
- How to log decisions (ADR) + evidence (Module E)

Rule: if you cannot onboard someone in 60 minutes, your documentation system is not real.

---

### 1.3 Role slicing (how to split work without collisions)

Slice by interfaces and artifacts, not vague “help me.”

Good slices:

- Own the telemetry schema + parser tests
- Own the PCB bring-up procedure + evidence pack
- Own the mechanical enclosure iteration + fit tests
- Own the CI pipeline + release packaging

Bad slice:

- Help with firmware

---

### 1.4 Handoffs (definition of done for collaboration)

Every handoff must include:

- What changed (diff summary)
- How to reproduce
- What remains unknown
- Evidence links
- Next step suggestion

If a handoff does not include reproduction steps, it is not a handoff. It is a message.

---

### 1.5 What good collaboration looks like

- Small PRs (reviewable)
- Evidence for claims (screenshots/logs)
- Decisions recorded (ADR)
- Respectful disagreement with data
- Predictable cadence (weekly review + demo)

### Solo‑to‑Team checklist

- [ ]  Onboarding kit exists and is up to date
- [ ]  Work is sliced into owned artifacts
- [ ]  Handoffs include repro steps + evidence
- [ ]  Decisions are logged, not re-litigated
- [ ]  WIP limits enforced

---

## 2) Time & Energy Management

**Scope**: focus blocks, weekly review, anti-chaos habits.

### 2.1 Bandwidth beats todo lists

Engineering work requires deep focus. If your schedule is meetings + pings, you will work all week and ship nothing.

---

### 2.2 The three-layer time system

**Layer 1 — Deep work blocks (primary)**

- 60–120 minute blocks
- One objective
- One artifact output

**Layer 2 — Shallow work windows**

- Email, messages, admin
- Parts ordering, quick reviews

**Layer 3 — Recovery**

- Breaks, walk, sleep
- Prevents quality collapse

Rule: protect deep work like you protect meetings.

---

### 2.3 Focus block rules (non-negotiable)

- Start with a written objective: “By end of block, X exists.”
- Prep inputs first (docs open, repo ready, instruments ready)
- No multitasking
- End with: result + next step + log/evidence saved

---

### 2.4 Weekly review (15–30 minutes that prevents drift)

Agenda:

1. What shipped last week? (evidence-based)
2. What broke? (root cause)
3. What are the top three risks now?
4. What is the next milestone and pass criteria?
5. What gets deprioritized?

Rule: if you do not deprioritize, you are lying about priorities.

---

### 2.5 Anti-chaos habits

Daily:

- 5 minutes: update Doing/Blocked
- Log decisions that happened
- Capture evidence for progress

Weekly:

- Archive old artifacts
- Check parts lead times
- Run a restore test quarterly (Module I)

### Time & Energy checklist

- [ ]  Deep work blocks exist on the calendar
- [ ]  WIP capped (≤3 active tasks)
- [ ]  Weekly review happens
- [ ]  Each block ends with an artifact + log entry
- [ ]  Shallow work batched, not scattered

---

## 3) Postmortems & Learning Culture

**Scope**: blameless write-ups, actions, regression tracking.

### 3.1 Postmortems prevent repeats

If something fails and you do not learn, you will repeat it.

Blameless means:

- Focus on system causes, not who messed up

But blameless is not consequence-free. You still create corrective actions.

---

### 3.2 When to run a postmortem

Run one when:

- Schedule slipped meaningfully
- A failure required a redesign
- A bug escaped into a demo/customer
- Data was corrupted or lost
- A safety issue occurred
- A week was burned on confusion

Rule: if it cost more than four hours, it deserves a postmortem.

---

### 3.3 Postmortem structure

- What happened (timeline)
- Impact (time/cost/user harm)
- Root causes (not symptoms)
- What went well
- What did not
- Action items (owner + due date)
- Regression test added? (yes/no)
- Decision log updates

Rule: no postmortem ends without at least one prevention mechanism (test, checklist, or process change).

---

### 3.4 Regression tracking (so fixes stick)

Every meaningful fix should create at least one of:

- Test case
- Checklist item
- Design rule
- Monitoring alert

Otherwise the fix is temporary.

### Postmortem checklist

- [ ]  Timeline and impact documented
- [ ]  Root cause identified
- [ ]  Action items assigned with due dates
- [ ]  Regression prevention added
- [ ]  Linked to ADR/test evidence

---

## 4) Communication for Engineers

**Scope**: status updates, demos, evidence, visuals.

### 4.1 Communication is a technical skill

Most engineering communication fails because it is:

- Too detailed without structure, or
- Too vague without evidence

Goal: deliver the right information at the right resolution.

---

### 4.2 Status updates that land

Status = progress + evidence + blockers + ask

Template:

- What shipped (with links)
- Current focus
- Blockers/risks
- Decisions needed (if any)
- Next milestone + ETA range

Rule: done must include evidence, not a claim.

---

### 4.3 Demoing evidence

Best demo pattern:

- Show pass criteria
- Show evidence (plot/log/video)
- Explain what it means
- State remaining risks

Avoid:

- It seems stable
- I think it’s fine

---

### 4.4 Visuals that persuade

Use:

- Block diagrams
- Sequence diagrams for protocols
- One plot per claim
- Labeled screenshots with arrows

Minimum standard:

- Title + date
- Version identifiers
- Measurement context

### Communication checklist

- [ ]  Status updates include evidence links
- [ ]  Claims tied to measurable criteria
- [ ]  Visuals labeled and contextual
- [ ]  Asks explicit (what you need, by when)